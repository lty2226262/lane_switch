# NuPlan dataset configuration
# Processed cameras:
#   idx    camera    original size    egocar_visible
#    0     CAM_F0    (1080, 1920)     False
#    1     CAM_L0    (1080, 1920)     True
#    2     CAM_R0    (1080, 1920)     True
#    3     CAM_L1    (1080, 1920)     False
#    4     CAM_R1    (1080, 1920)     False
#    5     CAM_L2    (1080, 1920)     True
#    6     CAM_R2    (1080, 1920)     True
#    7     CAM_B0    (1080, 1920)     False

data:
  data_root: data/nuplan/processed/mini # data root for the dataset
  dataset: nuplan # dataset type
  scene_idx: 2021.05.12.22.00.38_veh-35_01008_01518 # which scene to use, specific to NuPlan's naming convention
  start_timestep: 0 # which timestep to start from
  end_timestep: -1 # which timestep to end at, -1 means the last timestep
  preload_device: cuda # choose from ["cpu", "cuda"], cache the data on this device
  pixel_source: # image source and object annotations
    type: datasets.nuplan.nuplan_sourceloader.NuPlanPixelSource
    cameras: [0] # which cameras to use
    downscale_when_loading: [2] # the size of the images to load
    downscale: 1 # downscale factor wrt to the downscale_when_loading
    undistort: False # whether to undistort the images
    test_image_stride: 0 # use every Nth timestep for the test set. if 0, use all images for training and none for testing
    load_sky_mask: True # whether to load sky mask
    load_dynamic_mask: True # whether to load dynamic mask
    load_objects: True # whether to load object bounding boxes
    load_smpl: True # whether to load SMPL template for pedestrians
    sampler: # error based image sampler
      buffer_downscale: 8 # downscale factor for the buffer wrt load_size
      buffer_ratio: 0.5 # the percentage of images sampled according to the error buffer
      start_enhance_weight: 3 # give more chance to sample starting frames, which usually have more errors
  lidar_source: # everything related to "lidar" --- from lidar points
    type: datasets.nuplan.nuplan_sourceloader.NuPlanLiDARSource
    load_lidar: True # whether to load lidar
    # ---- compute aabb from lidar ---- #
    # if load_lidar is True, we compute aabb from lidar, otherwise we compute aabb from cameras
    # 1) downsample lidar by random sampling to 1/lidar_downsample_factor number of points
    # 2) compute aabb from the downsampled lidar points by using the percentile of lidar_percentiles 
    lidar_downsample_factor: 4 # downsample lidar by this factor to compute percentile
    lidar_percentile: 0.02 # percentile to compute aabb from lidar

render:
  fps: 10 # training fps
  render_full: true # whether to render the full video during evaluation
  render_test: true # whether to render the test set during evaluation
  render_novel: # novel view synthesis settings
    traj_types:
    - fixed_offset # trajectory type for novel view synthesis
    fps: 10 # fps for novel view synthesis
    offset: 3.0 # offset distance in meters
  vis_lidar: false # whether to visualize lidar points
  vis_sky: false # whether to visualize sky mask
  vis_error: false # whether to visualize error map

refiner:
  pretrained_id: nvidia/difix_ref

trainer:
  optim:
    num_iters: 300
  losses:
    lateral_refine_weight: 1.0 # weight for lateral refine loss

logging:
  vis_freq: 300 # visualization frequency
  print_freq: 100 # print frequency
  saveckpt_freq: 150 # checkpoint saving frequency
  save_seperate_video: True # whether to save separate videos for each scene